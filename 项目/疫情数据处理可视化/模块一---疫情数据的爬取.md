# 一、背景介绍
 新型冠状病毒感染的肺炎疫情爆发后，对人们的生活产生很大的影响。当前感染人数依然在不断变化。每天国家卫健委和各大新闻媒体都会公布疫情的数据，包括累计确诊人数、现有确诊人数等。
这次我们就使用Python开发网络爬虫，对新冠肺炎的疫情数据（包括当日的实时数据）进行采集。

# 二、我们选择网站爬取
1.网站有网易的，有腾讯的，由国家卫建委的等等很多，这个时候我们选择哪个来进行爬取呀！那我肯定是选个格式好的易于爬取的来进行爬取。这里我用python库爬取，也有用框架的，我也在学习，不要认为现在自己会很多，主要是你只要不断学习你就会很多了。

2.北京市卫健委数据用的文本，而湖南省卫健委数据用图片肯定都不太好采集，这时候伟大的网易来了，他的数据及其好，格式也好，数据是以字典数据存储，所以爬下来也好存呀，来吧。先看下网站，推荐使用火狐浏览器,可以查看json数据格式很对，Google都不行。
现在查看下数据，然后跟着分析，用他的url等数据。
![在这里插入图片描述](https://github.com/xiaohan99/xiaohan99.github.io/blob/master/%E5%9B%BE%E7%89%87/%E7%96%AB%E6%83%85%E7%88%AC%E5%8F%96%E5%88%86%E6%9E%90%E5%8F%AF%E8%A7%86%E5%8C%96/%E7%BD%91%E6%98%93%E7%BD%91%E9%A1%B5%E6%95%B0%E6%8D%AE1.png)

![在这里插入图片描述](https://github.com/xiaohan99/xiaohan99.github.io/blob/master/%E5%9B%BE%E7%89%87/%E7%96%AB%E6%83%85%E7%88%AC%E5%8F%96%E5%88%86%E6%9E%90%E5%8F%AF%E8%A7%86%E5%8C%96/%E7%BD%91%E6%98%93%E7%BD%91%E9%A1%B5%E6%95%B0%E6%8D%AE2.png)
![在这里插入图片描述](https://github.com/xiaohan99/xiaohan99.github.io/blob/master/%E5%9B%BE%E7%89%87/%E7%96%AB%E6%83%85%E7%88%AC%E5%8F%96%E5%88%86%E6%9E%90%E5%8F%AF%E8%A7%86%E5%8C%96/json%E6%95%B0%E6%8D%AE.png)
# 三、分析爬取测试
1.接下来开始请求，首先导入使用的包，使用request进行网页请求，使用pandas保存数据。
>import requests
import pandas as pd
import time 
import json

2.设置请求头，伪装为浏览器
>headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'
}

3.发起请求，将找到的第一个数据源作为请求目标。
>url = 'https://c.m.163.com/ug/api/wuhan/app/data/list-total'   # 定义要访问的地址

>r = requests.get(url, headers=headers)  # 使用requests发起请求
print(r.status_code)  # 查看请求状态
print(type(r.text))  # 查看数据类型 
print(len(r.text))    #查看数据

4.可以看到返回后的内容是一个几十万长度的字符串，由于字符串格式不方便进行分析，并且在网页预览中发现数据为类似字典的json格式，所以我们将其转为json格式。

>data_json = json.loads(r.text)
print(data_json.keys()) #输出查看json

5.根据网站可以看出在data中存放着我们需要的数据，因此我们取出数据。
>data = data_json['data'] # 取出json中的数据
print(data.keys()) #输出进行查看

6.数据中总共有四个键，每个键存储着不同的内容，我们输出一看就懂就是更新时间、实时等等名称。
首先我们抓取全国各省的实时数据，在areaTree键值对中，存放着世界各地的实时数据，areaTree是一个列表，每一个元素都是一个国家的数据，每一个元素的children是各国家省份的数据。 我们首先找到中国各省的实时数据，如下图所示。

![在这里插入图片描述](https://github.com/xiaohan99/xiaohan99.github.io/blob/master/%E5%9B%BE%E7%89%87/%E7%96%AB%E6%83%85%E7%88%AC%E5%8F%96%E5%88%86%E6%9E%90%E5%8F%AF%E8%A7%86%E5%8C%96/%E7%BD%91%E6%98%93%E7%BD%91%E9%A1%B5%E6%95%B0%E6%8D%AE3.png)

6.逐个拆解数据，我们已经了解children为下一地级数据，我们只分析到省单位，因此各省的children数据不采集；extData为空值，也不采集，最后具体采集方法如下：
不需要采集的数据：children、extData。
需要采集的数据：由于数据中today和total嵌套着字典，因此不能直接获取，对于id、lastUpdateTime、name、可以直接取出为一个数据，today为一个数据，total为一个数据，最后三个数据合并为一个数据。
>获取id、lastUpdateTime、name ，接着获取today数据total数据，就开始怕取了。
>这样爬取可能会不太友好，直接将他封装在函数里面后面世界怕去的时候直接变一下数据源不就可以直接调用了，所以真正代码写在def中包括存储也是，方便以后使用。

>	测试代码：
>info = pd.DataFrame(data_province)[['id','lastUpdateTime','name']]
print(info.head()) #head()这个是输出前五行进行查看
	# 获取today中的数据
today_data = pd.DataFrame([province['today'] for province in data_sf ])
	# print(today_data.head())
['today_'+i for i in today_data.columns]
today_data.columns = ['today_'+i for i in today_data.columns] # 由于today中键名和total键名相同，因此需要修改列名称
	# print(today_data.head())
	# 获取total中的数据
total_data = pd.DataFrame([province['total'] for province in data_sf ])
total_data.columns = ['total_'+i for i in total_data.columns]
	# print(total_data.head())
pd.concat([info,total_data,today_data],axis=1).head() # 将三个数据合并

代码来了：
因为要做实时分析就做了一个全国历史数据，然后也没用到：

```python
import requests
import pandas as pd
import time
import json

url= "https://c.m.163.com/ug/api/wuhan/app/data/list-total?"
header={
           "user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36"
           }
print("准备部分:\n")
# 分析
# 使用get获取请求

requests=requests.get(url,headers=header)
print(requests.status_code)     #查看请求状态
print(type(requests.text))      #查看数据类型
print(len(requests.text))       #数据

#网页预览中发现数据为类似字典的json格式，我们转为json格式。
data_json=json.loads(requests.text)
data_json.keys()   #转化输出  键 值不输出
print("爬取转化输出键值对：\n",data_json)

data=data_json['data']  #取出json数据
data.keys()
#print("取出josn数据：",data.keys())   #此时data数据已经变了+了keys()改为上方


#根据网页进行逐渐层次进行写入
data_sf=data["areaTree"][2]["children"]
print("爬取各个省份：\n",data_sf)
print("转化为列表:\n",type(data_sf))
print("取出键，2里面的头数据：\n",data_sf[0].keys())

#输出前五条数据查看一下
print('取出id、lastUpdateTime、name前五位数据：\n')
for i in range(len(data_sf)):
    print(data_sf[i]["id"],data_sf[i]["name"],data_sf[i]["lastUpdateTime"])
    if i==5:
        break
        
'''
print("开始爬取：\n")
# 开始
# 获取id、lastUpdateTime、name
info = pd.DataFrame(data_sf)[['id','lastUpdateTime','name']]
# print(info.head())

# 获取today中的数据
today_data = pd.DataFrame([province['today'] for province in data_sf ])
# print(today_data.head())

['today_'+i for i in today_data.columns]
today_data.columns = ['today_'+i for i in today_data.columns] # 由于today中键名和total键名相同，因此需要修改列名称
# print(today_data.head())

# 获取total中的数据
total_data = pd.DataFrame([province['total'] for province in data_sf ])
total_data.columns = ['total_'+i for i in total_data.columns]
# print(total_data.head())

pd.concat([info,total_data,today_data],axis=1).head() # 将三个数据合并

'''

# 将上面提取数据的方法封装为函数
#这里只是将上面封装在一个函数内，所以上面的便是演示了
def get_data(data, info_list):
    info = pd.DataFrame(data)[info_list]  # 主要信息

    today_data = pd.DataFrame([i['today'] for i in data])  # 生成today的数据
    today_data.columns = ['today_' + i for i in today_data.columns]  # 修改列名

    total_data = pd.DataFrame([i['total'] for i in data])  # 生成total的数据
    total_data.columns = ['total_' + i for i in total_data.columns]  # 修改列名

    return pd.concat([info, total_data, today_data], axis=1)  # info、today和total横向合并最终得到汇总的数据

today_province = get_data(data_sf,['id','lastUpdateTime','name'])  #调用get_data
print(today_province.head())

def save_data(data,name): # 定义保存数据方法
    file_name = name+'_'+time.strftime('%Y-%m-%d',time.localtime(time.time()))+'.csv'   #数据存储名称
    data.to_csv(file_name,index=None,encoding='utf_8_sig')   #  存数据重点
    print(file_name+' 保存成功！')   #使用上面file_name然后在输出器上打印保存成功
print(save_data(today_province,'全国各省实时数据'))  #调用sava_data(参数)


print("\n","国外实时数据","#"*100,"\n")

areaTree = data['areaTree'] # 取出areaTree
print(areaTree[0]) # 查看第一个国家的数据

for i in range(len(areaTree)):  # 查看各国家名称和更新时间
    print(areaTree[i]['name'],areaTree[i]['lastUpdateTime'])
    if i == 5:
        break

today_world = get_data(areaTree,['id','lastUpdateTime','name'])     #  调用get_data函数
print(today_world.head())                        #   .head()是将输出前五条数据
print(save_data(today_world,'世界各国实时数据'))   #  调用了save_data保存方法

'''
print("\n","全国历史数据","#"*100,"\n")

data.keys()

chinaDayList = data['chinaDayList'] # 取出chinaDayList
print(type(chinaDayList)) # 查看chinaDayList的格式

print(chinaDayList[0])

alltime_China = get_data(chinaDayList,['date','lastUpdateTime'])    #  调用get_data函数
print(alltime_China.head())                      #   .head()是将输出前五条数据
print(save_data(alltime_China,'全国历史数据'))    #  调用了save_data保存方法

'''
```

![在这里插入图片描述](https://github.com/xiaohan99/xiaohan99.github.io/blob/master/%E5%9B%BE%E7%89%87/%E7%96%AB%E6%83%85%E7%88%AC%E5%8F%96%E5%88%86%E6%9E%90%E5%8F%AF%E8%A7%86%E5%8C%96/%E6%95%B0%E6%8D%AE1.png)
![在这里插入图片描述](https://github.com/xiaohan99/xiaohan99.github.io/blob/master/%E5%9B%BE%E7%89%87/%E7%96%AB%E6%83%85%E7%88%AC%E5%8F%96%E5%88%86%E6%9E%90%E5%8F%AF%E8%A7%86%E5%8C%96/%E6%95%B0%E6%8D%AE2.png)
提示：因为我们爬取的对象是网页上的数据，如果这些网页的数据内容改变我们也得去改变。
加油，还年轻呀！！！